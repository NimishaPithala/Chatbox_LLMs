# -*- coding: utf-8 -*-
"""Chatbox with Open Source LLMs.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FuBnl2r3taNcB_sbRKGb-bQHWVASeP6B
"""

pip install transformers

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

model_name = "facebook/blenderbot-400M-distill"

model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

"""1.Initialize object to store conversation history
2.Encode conversation history as a string
3.Fetch prompt from user
4.Tokenize prompt
5.Generate output from model using prompt and history
6.Decode output
7.Update conversation history
"""

conversation_history = []

history_string = "\n".join(conversation_history)
history_string

input_text ="hello"
input_text

inputs = tokenizer.encode_plus(history_string, input_text, return_tensors="pt")
inputs

tokenizer.pretrained_vocab_files_map

outputs = model.generate(**inputs)
outputs

response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()
response

conversation_history.append(input_text)
conversation_history.append(response)
conversation_history

while True:
    history_string = "\n".join(conversation_history)

    input_text = input("> ")

    inputs = tokenizer.encode_plus(history_string, input_text, return_tensors="pt")

    outputs = model.generate(**inputs)

    response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()

    conversation_history.append(input_text)
    conversation_history.append(response)

"""Backend"""

pip install flask

pip install flask_cors

from flask import Flask
from flask_cors import CORS		# newly added

app = Flask(__name__)
CORS(app)				# newly added

@app.route('/')
def home():
    return 'Hello, World!'

if __name__ == '__main__':
    app.run()

from flask import request
import json

{
    'prompt': 'message'
}

@app.route('/chatbot', methods=['POST'])
def handle_prompt():

    data = request.get_data(as_text=True)
    data = json.loads(data)
    input_text = data['prompt']

    history = "\n".join(conversation_history)

    inputs = tokenizer.encode_plus(history, input_text, return_tensors="pt")

    outputs = model.generate(**inputs, max_length= 60)

    response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()

    conversation_history.append(input_text)
    conversation_history.append(response)

    return response